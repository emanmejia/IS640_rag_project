{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Verification Notebook\n",
    "\n",
    "Run all cells in this notebook to verify your environment is set up correctly.\n",
    "\n",
    "## What This Tests:\n",
    "1. Python dependencies\n",
    "2. Ollama LLM connection\n",
    "3. Embedding model\n",
    "4. ChromaDB vector database\n",
    "5. Basic LLM interaction\n",
    "\n",
    "If all cells run successfully, you're ready to start the project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.9 | packaged by conda-forge | (main, Oct 22 2025, 23:12:41) [MSC v.1944 64 bit (AMD64)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ChromaDB imported successfully\n",
      "✓ Sentence Transformers imported successfully\n",
      "✓ Requests imported successfully\n",
      "\n",
      "✓ All dependencies imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test imports\n",
    "try:\n",
    "    import chromadb\n",
    "    print(\"✓ ChromaDB imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(\"✗ ChromaDB import failed:\", e)\n",
    "    print(\"  Install with: pip install chromadb\")\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"✓ Sentence Transformers imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(\"✗ Sentence Transformers import failed:\", e)\n",
    "    print(\"  Install with: pip install sentence-transformers\")\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    print(\"✓ Requests imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(\"✗ Requests import failed:\", e)\n",
    "    print(\"  Install with: pip install requests\")\n",
    "\n",
    "print(\"\\n✓ All dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 2: Ollama LLM Connection\n",
    "\n",
    "This test checks if Ollama is running and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ollama is running and accessible\n",
      "\n",
      "Available models:\n",
      "  - mistral:7b\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "OLLAMA_URL = \"http://127.0.0.1:11434\"\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    response = requests.get(f\"{OLLAMA_URL}/api/tags\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"✓ Ollama is running and accessible\")\n",
    "        \n",
    "        # List available models\n",
    "        models = response.json().get('models', [])\n",
    "        if models:\n",
    "            print(f\"\\nAvailable models:\")\n",
    "            for model in models:\n",
    "                print(f\"  - {model['name']}\")\n",
    "        else:\n",
    "            print(\"\\n⚠ No models found. You may need to pull a model.\")\n",
    "            print(\"  Run: docker exec ollama-mistral-offline ollama pull mistral\")\n",
    "    else:\n",
    "        print(f\"✗ Ollama returned status code: {response.status_code}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"✗ Cannot connect to Ollama at\", OLLAMA_URL)\n",
    "    print(\"\\nMake sure:\")\n",
    "    print(\"  1. Docker Desktop is running\")\n",
    "    print(\"  2. Ollama container is started\")\n",
    "    print(\"  3. Check with: docker ps\")\n",
    "    print(\"\\nSee COMMANDS.txt for Docker setup instructions.\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 3: Embedding Model\n",
    "\n",
    "This test loads a small embedding model and creates a test embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "(This may take a minute on first run)\n",
      "\n",
      "✓ Embedding model loaded successfully\n",
      "\n",
      "Test embedding:\n",
      "  Text: 'Hello, this is a test sentence.'\n",
      "  Embedding dimension: 384\n",
      "  First 5 values: [ 0.0542045   0.09602847  0.02270406  0.10747128 -0.01486251]\n",
      "\n",
      "✓ Embedding model is working correctly!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "print(\"(This may take a minute on first run)\\n\")\n",
    "\n",
    "try:\n",
    "    # Load a small, fast model\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    print(\"✓ Embedding model loaded successfully\")\n",
    "    \n",
    "    # Test embedding\n",
    "    test_text = \"Hello, this is a test sentence.\"\n",
    "    embedding = model.encode(test_text)\n",
    "    \n",
    "    print(f\"\\nTest embedding:\")\n",
    "    print(f\"  Text: '{test_text}'\")\n",
    "    print(f\"  Embedding dimension: {len(embedding)}\")\n",
    "    print(f\"  First 5 values: {embedding[:5]}\")\n",
    "    print(\"\\n✓ Embedding model is working correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading embedding model: {e}\")\n",
    "    print(\"\\nThis could mean:\")\n",
    "    print(\"  1. Not enough RAM (need at least 4GB free)\")\n",
    "    print(\"  2. No internet connection (needed for first download)\")\n",
    "    print(\"  3. Disk space issue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 4: ChromaDB Vector Database\n",
    "\n",
    "This test creates a temporary collection and tests basic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ChromaDB client initialized\n",
      "✓ Test collection created\n",
      "✓ Test documents added\n",
      "✓ Query executed successfully\n",
      "\n",
      "✓ ChromaDB is working correctly!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Use a temporary directory for testing\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "try:\n",
    "    # Initialize ChromaDB\n",
    "    client = chromadb.PersistentClient(path=temp_dir)\n",
    "    print(\"✓ ChromaDB client initialized\")\n",
    "    \n",
    "    # Create a test collection\n",
    "    collection = client.create_collection(name=\"test_collection\")\n",
    "    print(\"✓ Test collection created\")\n",
    "    \n",
    "    # Add some test documents\n",
    "    collection.add(\n",
    "        documents=[\"This is document 1\", \"This is document 2\"],\n",
    "        ids=[\"doc1\", \"doc2\"]\n",
    "    )\n",
    "    print(\"✓ Test documents added\")\n",
    "    \n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_texts=[\"document\"],\n",
    "        n_results=2\n",
    "    )\n",
    "    print(\"✓ Query executed successfully\")\n",
    "    \n",
    "    # Clean up\n",
    "    client.delete_collection(name=\"test_collection\")\n",
    "    print(\"\\n✓ ChromaDB is working correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ ChromaDB test failed: {e}\")\n",
    "finally:\n",
    "    # Clean up temp directory\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 5: Basic LLM Interaction\n",
    "\n",
    "This test sends a simple prompt to the LLM and gets a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LLM with a simple question...\n",
      "(This may take 10-30 seconds)\n",
      "\n",
      "Prompt: Answer in one sentence: What is 2+2?\n",
      "Response:  The sum of 2 and 2 is 4.\n",
      "\n",
      "✓ LLM is responding correctly!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "OLLAMA_URL = \"http://127.0.0.1:11434\"\n",
    "MODEL_NAME = \"mistral:7b\"  # Change if using a different model\n",
    "\n",
    "def test_llm(prompt: str) -> str:\n",
    "    \"\"\"Send a prompt to Ollama and get response.\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/api/generate\",\n",
    "            json={\n",
    "                \"model\": MODEL_NAME,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()['response']\n",
    "        else:\n",
    "            return f\"Error: Status code {response.status_code}\"\n",
    "    \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return \"Error: Cannot connect to Ollama. Is the container running?\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test the LLM\n",
    "print(\"Testing LLM with a simple question...\")\n",
    "print(\"(This may take 10-30 seconds)\\n\")\n",
    "\n",
    "test_prompt = \"Answer in one sentence: What is 2+2?\"\n",
    "response = test_llm(test_prompt)\n",
    "\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "if \"Error\" not in response:\n",
    "    print(\"\\n✓ LLM is responding correctly!\")\n",
    "else:\n",
    "    print(\"\\n✗ LLM test failed\")\n",
    "    print(\"\\nCheck:\")\n",
    "    print(\"  1. Ollama container is running: docker ps\")\n",
    "    print(\"  2. Correct model is installed\")\n",
    "    print(\"  3. See COMMANDS.txt for troubleshooting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test 6: File System Access\n",
    "\n",
    "This test verifies you can read the sample documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Documents folder found: ./docs/text\n",
      "\n",
      "✓ Found 1 text file(s):\n",
      "  - sample1.txt (135 bytes)\n",
      "\n",
      "✓ Successfully read sample1.txt\n",
      "  Content preview: Artificial intelligence is transforming the way organizations use data.\n",
      "This is a sample document fo...\n",
      "\n",
      "✓ File system access is working!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DOCS_FOLDER = \"./docs/text\"\n",
    "\n",
    "try:\n",
    "    docs_path = Path(DOCS_FOLDER)\n",
    "    \n",
    "    if not docs_path.exists():\n",
    "        print(f\"✗ Documents folder not found: {DOCS_FOLDER}\")\n",
    "        print(\"  Make sure you're running this from the project root directory\")\n",
    "    else:\n",
    "        print(f\"✓ Documents folder found: {DOCS_FOLDER}\")\n",
    "        \n",
    "        # List text files\n",
    "        text_files = list(docs_path.glob(\"*.txt\"))\n",
    "        \n",
    "        if text_files:\n",
    "            print(f\"\\n✓ Found {len(text_files)} text file(s):\")\n",
    "            for file in text_files:\n",
    "                # Get file size\n",
    "                size = file.stat().st_size\n",
    "                print(f\"  - {file.name} ({size:,} bytes)\")\n",
    "            \n",
    "            # Test reading one file\n",
    "            test_file = text_files[0]\n",
    "            with open(test_file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            print(f\"\\n✓ Successfully read {test_file.name}\")\n",
    "            print(f\"  Content preview: {content[:100]}...\")\n",
    "            print(\"\\n✓ File system access is working!\")\n",
    "        else:\n",
    "            print(f\"\\n⚠ No .txt files found in {DOCS_FOLDER}\")\n",
    "            print(\"  You'll need documents to test your RAG system\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error accessing files: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "If all tests passed, your environment is ready!\n",
    "\n",
    "### Next Steps:\n",
    "1. Read `STUDENT_PROJECT_GUIDE.md` for assignment details\n",
    "2. Prepare your own document collection\n",
    "3. Start working on the TODO tasks\n",
    "\n",
    "### If Any Tests Failed:\n",
    "- Check the error messages above\n",
    "- See `docker_starter.md` for Docker setup\n",
    "- See `COMMANDS.txt` for Docker commands\n",
    "- Ask for help if you're stuck\n",
    "\n",
    "**Good luck with your project!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Loaded 1 file(s): ['sample1.txt']\n",
      "Chroma collection ready.\n",
      "Embedding and adding to Chroma...\n",
      "Indexed documents.\n",
      "\n",
      "Retrieval results (top 3):\n",
      "  1. sample1.txt  (distance=1.4809)\n",
      "\n",
      "Querying Mistral via Ollama...\n",
      "\n",
      "=== Answer ===\n",
      " 1. The first document suggests that artificial intelligence (AI) is significantly changing how\n",
      "organizations manage and utilize data. 2. This statement was made in a document meant to test local\n",
      "RAG integration, which implies it may not directly relate to AI's broader impacts or applications\n",
      "but rather its specific role within the organization testing the integration.\n"
     ]
    }
   ],
   "source": [
    "# Mini-RAG: embed → store → retrieve → generate (Ollama)\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests, textwrap, os, json\n",
    "\n",
    "# --- Config ---\n",
    "DOCS_FOLDER = Path(\"./docs/text\")\n",
    "OLLAMA_URL = \"http://127.0.0.1:11434\"\n",
    "MODEL_NAME = \"mistral:7b\"  # confirmed from /api/tags\n",
    "COLLECTION_NAME = \"rag_demo\"\n",
    "\n",
    "# --- Load/embed model ---\n",
    "print(\"Loading embedding model...\")\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# --- Collect docs ---\n",
    "files = sorted(DOCS_FOLDER.glob(\"*.txt\"))\n",
    "assert files, f\"No .txt files in {DOCS_FOLDER}\"\n",
    "docs = [f.read_text(encoding=\"utf-8\") for f in files]\n",
    "ids = [f.name for f in files]\n",
    "print(f\"Loaded {len(files)} file(s):\", [f.name for f in files])\n",
    "\n",
    "# --- Chroma: create/reset collection ---\n",
    "client = chromadb.PersistentClient(path=\"./.chroma\")  # persisted locally\n",
    "try:\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "except Exception:\n",
    "    pass\n",
    "col = client.create_collection(name=COLLECTION_NAME)\n",
    "print(\"Chroma collection ready.\")\n",
    "\n",
    "# --- Add embeddings ---\n",
    "print(\"Embedding and adding to Chroma...\")\n",
    "embeddings = embedder.encode(docs, convert_to_numpy=True).tolist()\n",
    "col.add(documents=docs, embeddings=embeddings, ids=ids)\n",
    "print(\"Indexed documents.\")\n",
    "\n",
    "# --- Simple retriever ---\n",
    "def retrieve(query, k=3):\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True).tolist()\n",
    "    res = col.query(query_embeddings=q_emb, n_results=k)\n",
    "    hits = list(zip(res[\"ids\"][0], res[\"documents\"][0], res[\"distances\"][0]))\n",
    "    return hits\n",
    "\n",
    "# --- LLM call (Ollama /api/generate) ---\n",
    "def ollama_generate(prompt: str) -> str:\n",
    "    r = requests.post(\n",
    "        f\"{OLLAMA_URL}/api/generate\",\n",
    "        json={\"model\": MODEL_NAME, \"prompt\": prompt, \"stream\": False},\n",
    "        timeout=120,\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"response\"].strip()\n",
    "\n",
    "# --- Ask a question ---\n",
    "question = \"Summarize the key ideas from the documents in two sentences.\"\n",
    "hits = retrieve(question, k=3)\n",
    "context = \"\\n\\n---\\n\\n\".join([f\"[{i+1}] {h[1]}\" for i, h in enumerate(hits)])\n",
    "\n",
    "prompt = f\"\"\"Use the context to answer the question concisely.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"\\nRetrieval results (top 3):\")\n",
    "for i, (doc_id, _, dist) in enumerate(hits, 1):\n",
    "    print(f\"  {i}. {doc_id}  (distance={dist:.4f})\")\n",
    "\n",
    "print(\"\\nQuerying Mistral via Ollama...\")\n",
    "answer = ollama_generate(prompt)\n",
    "print(\"\\n=== Answer ===\\n\", textwrap.fill(answer, width=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag3_313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
