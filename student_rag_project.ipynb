{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student RAG Project - Guided Implementation\n",
    "\n",
    "## Welcome!\n",
    "\n",
    "In this project, you'll build a **RAG (Retrieval-Augmented Generation)** system that can answer questions about your documents.\n",
    "\n",
    "### What You'll Learn:\n",
    "- âœ… File I/O (reading documents)\n",
    "- âœ… String manipulation (text chunking)\n",
    "- âœ… Functions and parameters\n",
    "- âœ… Lists and dictionaries\n",
    "- âœ… Loops and conditionals\n",
    "- âœ… Basic calculations and statistics\n",
    "\n",
    "### What's Provided for You:\n",
    "- âœ… Embedding model (converts text to numbers)\n",
    "- âœ… Vector database (stores and searches embeddings)\n",
    "- âœ… LLM connection (generates answers)\n",
    "\n",
    "### Your Tasks:\n",
    "You'll complete **TODO sections** marked with `# TODO:` comments.\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking setup...\n",
      "âœ“ chromadb is installed\n",
      "âœ“ sentence_transformers is installed\n",
      "âœ“ requests is installed\n",
      "\n",
      "âœ“ All required packages are installed!\n",
      "You're ready to start!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the pre-built helper module\n",
    "from rag_helpers import (\n",
    "    EmbeddingModel,\n",
    "    VectorDatabase,\n",
    "    LLM,\n",
    "    Timer,\n",
    "    print_separator,\n",
    "    print_search_results,\n",
    "    print_rag_answer,\n",
    "    check_setup\n",
    ")\n",
    "\n",
    "# Import standard Python libraries you'll use\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "# Check if everything is installed correctly\n",
    "check_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration\n",
    "\n",
    "Set up the basic settings for your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Documents folder: ./docs/text\n",
      "  Chunk size: 500 characters\n",
      "  Overlap: 50 characters\n",
      "  Top-K results: 3\n"
     ]
    }
   ],
   "source": [
    "# TODO: Change this to point to YOUR documents folder\n",
    "DOCS_FOLDER = \"./docs/text\"\n",
    "\n",
    "# Chunking settings (you can experiment with these!)\n",
    "CHUNK_SIZE = 500      # How many characters per chunk\n",
    "OVERLAP = 50          # How many characters overlap between chunks\n",
    "\n",
    "# How many results to retrieve for each query\n",
    "TOP_K = 3\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Documents folder: {DOCS_FOLDER}\")\n",
    "print(f\"  Chunk size: {CHUNK_SIZE} characters\")\n",
    "print(f\"  Overlap: {OVERLAP} characters\")\n",
    "print(f\"  Top-K results: {TOP_K}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #1: Document Loading\n",
    "\n",
    "**Your Task:** Write a function to load all text files from a folder.\n",
    "\n",
    "**What to do:**\n",
    "1. Loop through all `.txt` files in the folder\n",
    "2. Read each file's content\n",
    "3. Store the content and filename in a dictionary\n",
    "4. Return a list of these dictionaries\n",
    "\n",
    "**Python concepts:** File I/O, loops, dictionaries, lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "def load_documents(folder_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load all text documents from a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path: Path to folder containing .txt files\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries, each containing:\n",
    "        - 'content': the text content of the file\n",
    "        - 'filename': the name of the file\n",
    "    \"\"\"\n",
    "    documents = []  # Start with empty list\n",
    "    folder = Path(folder_path)\n",
    "\n",
    "    # Loop through all .txt files in the folder\n",
    "    for file_path in folder.glob(\"*.txt\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        doc_info = {\n",
    "            \"content\": content,\n",
    "            \"filename\": file_path.name\n",
    "        }\n",
    "        documents.append(doc_info)\n",
    "\n",
    "    print(f\"âœ“ Loaded {len(documents)} documents\")\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\Mejia\\Dev\\Rag\\IS640_rag_project\n",
      "\n",
      "Checking candidate paths for .txt files:\n",
      "\n",
      "Path: docs/text\n",
      "  Exists?: True\n",
      "  .txt files found: [WindowsPath('docs/text/arrozConGandules.txt'), WindowsPath('docs/text/coquito.txt'), WindowsPath('docs/text/empanadilla.txt'), WindowsPath('docs/text/panDeAgua.txt'), WindowsPath('docs/text/pastelon.txt'), WindowsPath('docs/text/pernil.txt'), WindowsPath('docs/text/sample1.txt'), WindowsPath('docs/text/sofrito.txt')]\n",
      "\n",
      "Path: ./docs/text\n",
      "  Exists?: True\n",
      "  .txt files found: [WindowsPath('docs/text/arrozConGandules.txt'), WindowsPath('docs/text/coquito.txt'), WindowsPath('docs/text/empanadilla.txt'), WindowsPath('docs/text/panDeAgua.txt'), WindowsPath('docs/text/pastelon.txt'), WindowsPath('docs/text/pernil.txt'), WindowsPath('docs/text/sample1.txt'), WindowsPath('docs/text/sofrito.txt')]\n",
      "\n",
      "Path: docs\n",
      "  Exists?: True\n",
      "  .txt files found: []\n",
      "\n",
      "Path: ./docs\n",
      "  Exists?: True\n",
      "  .txt files found: []\n",
      "\n",
      "Using DOCS_FOLDER = 'docs/text'\n",
      "âœ“ Loaded 8 documents\n",
      "\n",
      "First document: arrozConGandules.txt\n",
      "Content preview: Arroz Con Gandules (Puerto Rican Rice with Pigeon Peas)\n",
      "Jannese\n",
      "48â€“61 minutes\n",
      "\n",
      "December 4, 2022\t\n",
      "\n",
      "Thereâ€™s nothing more quintessentially Puerto Rican than arroz con gandules.  \n",
      "\n",
      "This post may contain a...\n"
     ]
    }
   ],
   "source": [
    "# ðŸ›  Debug + Load Documents Cell\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "\n",
    "# Check some common candidate paths\n",
    "candidate_paths = [\"docs/text\", \"./docs/text\", \"docs\", \"./docs\"]\n",
    "print(\"\\nChecking candidate paths for .txt files:\")\n",
    "for cp in candidate_paths:\n",
    "    folder = Path(cp)\n",
    "    print(f\"\\nPath: {cp}\")\n",
    "    print(\"  Exists?:\", folder.exists())\n",
    "    if folder.exists():\n",
    "        txt_files = list(folder.glob(\"*.txt\"))\n",
    "        print(\"  .txt files found:\", txt_files)\n",
    "    else:\n",
    "        print(\"  .txt files found: []\")\n",
    "\n",
    "# Try to auto-select a good DOCS_FOLDER based on what we just saw\n",
    "if Path(\"docs/text\").exists() and list(Path(\"docs/text\").glob(\"*.txt\")):\n",
    "    DOCS_FOLDER = \"docs/text\"\n",
    "elif Path(\"./docs/text\").exists() and list(Path(\"./docs/text\").glob(\"*.txt\")):\n",
    "    DOCS_FOLDER = \"./docs/text\"\n",
    "elif Path(\"docs\").exists() and list(Path(\"docs\").glob(\"*.txt\")):\n",
    "    DOCS_FOLDER = \"docs\"\n",
    "elif Path(\"./docs\").exists() and list(Path(\"./docs\").glob(\"*.txt\")):\n",
    "    DOCS_FOLDER = \"./docs\"\n",
    "\n",
    "print(f\"\\nUsing DOCS_FOLDER = {DOCS_FOLDER!r}\")\n",
    "\n",
    "# Now call your existing load_documents() with the detected folder\n",
    "documents = load_documents(DOCS_FOLDER)\n",
    "\n",
    "# Display first document (if any were loaded)\n",
    "if documents:\n",
    "    print(f\"\\nFirst document: {documents[0]['filename']}\")\n",
    "    print(f\"Content preview: {documents[0]['content'][:200]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸  No documents loaded! Check your folder path again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 8 documents\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents(DOCS_FOLDER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #2: Text Chunking Function\n",
    "\n",
    "**Your Task:** Write a function to split long text into smaller chunks with overlap.\n",
    "\n",
    "**Why?** Long documents are too big for embeddings. We need to split them into smaller pieces.\n",
    "\n",
    "**What to do:**\n",
    "1. Start at the beginning of the text\n",
    "2. Take a chunk of `chunk_size` characters\n",
    "3. Move forward by `chunk_size - overlap` characters\n",
    "4. Repeat until you reach the end\n",
    "\n",
    "**Python concepts:** String slicing, loops, lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    position = 0\n",
    "\n",
    "    # Safety: ensure overlap is smaller than chunk size\n",
    "    step = chunk_size - overlap\n",
    "    if step <= 0:\n",
    "        raise ValueError(\"Overlap must be smaller than chunk_size\")\n",
    "\n",
    "    # Main loop\n",
    "    while position < len(text):\n",
    "        chunk = text[position : position + chunk_size]\n",
    "\n",
    "        # Only add non-empty chunks\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        # Move forward\n",
    "        position += step\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text length: 800 characters\n",
      "Number of chunks: 10\n",
      "\n",
      "First chunk: This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This\n",
      "Second chunk: This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This\n"
     ]
    }
   ],
   "source": [
    "# Test your chunking function\n",
    "test_text = \"This is a test. \" * 50  # Create a long test string\n",
    "test_chunks = chunk_text(test_text, chunk_size=100, overlap=20)\n",
    "\n",
    "print(f\"Test text length: {len(test_text)} characters\")\n",
    "print(f\"Number of chunks: {len(test_chunks)}\")\n",
    "print(f\"\\nFirst chunk: {test_chunks[0]}\")\n",
    "if len(test_chunks) > 1:\n",
    "    print(f\"Second chunk: {test_chunks[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #3: Process All Documents into Chunks\n",
    "\n",
    "**Your Task:** Use your chunking function to split ALL documents into chunks and create metadata.\n",
    "\n",
    "**What to do:**\n",
    "1. Loop through each document\n",
    "2. Chunk the document's content\n",
    "3. For each chunk, create metadata (which file it came from, which chunk number)\n",
    "4. Store everything in a list\n",
    "\n",
    "**Python concepts:** Nested loops, dictionaries, enumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(documents: List[Dict[str, str]],\n",
    "                     chunk_size: int = 500,\n",
    "                     overlap: int = 50) -> tuple:\n",
    "    \"\"\"\n",
    "    Process all documents into chunks with metadata.\n",
    "    \"\"\"\n",
    "    chunk_texts = []\n",
    "    chunk_metadatas = []\n",
    "\n",
    "    for doc in documents:\n",
    "        # Get document content and filename\n",
    "        content = doc[\"content\"]\n",
    "        filename = doc[\"filename\"]\n",
    "\n",
    "        # Chunk the document using your chunk_text function\n",
    "        chunks = chunk_text(content, chunk_size=chunk_size, overlap=overlap)\n",
    "\n",
    "        # For each chunk:\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk_texts.append(chunk)\n",
    "\n",
    "            metadata = {\n",
    "                \"source\": filename,\n",
    "                \"chunk_id\": idx\n",
    "            }\n",
    "            chunk_metadatas.append(metadata)\n",
    "\n",
    "    print(f\"âœ“ Created {len(chunk_texts)} chunks from {len(documents)} documents\")\n",
    "    return chunk_texts, chunk_metadatas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 148 chunks from 8 documents\n",
      "\n",
      "Example chunk:\n",
      "  Source: arrozConGandules.txt\n",
      "  Chunk ID: 0\n",
      "  Text: Arroz Con Gandules (Puerto Rican Rice with Pigeon Peas)\n",
      "Jannese\n",
      "48â€“61 minutes\n",
      "\n",
      "December 4, 2022\t\n",
      "\n",
      "Thereâ€™s nothing more quintessentially Puerto Rican than arroz con gandules.  \n",
      "\n",
      "This post may contain a...\n"
     ]
    }
   ],
   "source": [
    "chunk_texts, chunk_metadatas = process_documents(documents, CHUNK_SIZE, OVERLAP)\n",
    "\n",
    "if chunk_texts:\n",
    "    print(f\"\\nExample chunk:\")\n",
    "    print(f\"  Source: {chunk_metadatas[0]['source']}\")\n",
    "    print(f\"  Chunk ID: {chunk_metadatas[0]['chunk_id']}\")\n",
    "    print(f\"  Text: {chunk_texts[0][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Built: Create Embeddings and Store in Database\n",
    "\n",
    "This part uses the pre-built helpers. Just run these cells - no coding needed! âœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model...\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "âœ“ Model loaded!\n",
      "\n",
      "Creating embeddings...\n",
      "Embedding 148 texts...\n",
      "âœ“ Complete!\n",
      "âœ“ Created 148 embeddings\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the embedding model (pre-built)\n",
    "print(\"Initializing embedding model...\")\n",
    "embedder = EmbeddingModel()\n",
    "\n",
    "# Create embeddings for all chunks (pre-built)\n",
    "print(\"\\nCreating embeddings...\")\n",
    "embeddings = embedder.embed_multiple(chunk_texts)\n",
    "print(f\"âœ“ Created {len(embeddings)} embeddings\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing vector database...\n",
      "âœ“ Vector database initialized\n",
      "  Collection: student_rag\n",
      "  Current documents: 252\n",
      "\n",
      "Adding chunks to database...\n",
      "âœ“ Added 148 chunks to database\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector database (pre-built)\n",
    "print(\"Initializing vector database...\")\n",
    "vector_db = VectorDatabase()\n",
    "\n",
    "# Add chunks to database (pre-built)\n",
    "print(\"\\nAdding chunks to database...\")\n",
    "vector_db.add_chunks(chunk_texts, embeddings, chunk_metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Ollama LLM...\n",
      "âœ“ LLM initialized: mistral:7b at http://127.0.0.1:11434\n",
      "\n",
      "Testing LLM connection...\n",
      "âœ“ LLM is working!\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM connection (pre-built)\n",
    "print(\"Connecting to Ollama LLM...\")\n",
    "llm = LLM()\n",
    "\n",
    "# Test the connection\n",
    "print(\"\\nTesting LLM connection...\")\n",
    "if llm.test_connection():\n",
    "    print(\"âœ“ LLM is working!\")\n",
    "else:\n",
    "    print(\"âš ï¸  LLM connection failed! Make sure Docker container is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #4: RAG Query Function\n",
    "\n",
    "**Your Task:** Write the main RAG function that ties everything together!\n",
    "\n",
    "**What to do:**\n",
    "1. Embed the user's question\n",
    "2. Search the database for similar chunks\n",
    "3. Build a prompt with the retrieved context\n",
    "4. Ask the LLM to answer based on the context\n",
    "5. Return the answer and metadata\n",
    "\n",
    "**Python concepts:** Functions, string formatting, dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question: str, top_k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG (Retrieval-Augmented Generation).\n",
    "    \"\"\"\n",
    "\n",
    "    # Start timer\n",
    "    timer = Timer()\n",
    "    timer.start()\n",
    "\n",
    "    # Step 1: Embed question\n",
    "    query_embedding = embedder.embed_text(question)\n",
    "\n",
    "    # Step 2: Search database\n",
    "    results = vector_db.search(query_embedding, top_k=top_k)\n",
    "\n",
    "    # Step 3: Extract results\n",
    "    # results[\"documents\"] and results[\"metadatas\"] are lists of lists\n",
    "    retrieved_chunks = results[\"documents\"][0] if results[\"documents\"] else []\n",
    "    retrieved_metadata = results[\"metadatas\"][0] if results[\"metadatas\"] else []\n",
    "\n",
    "    # Step 4: Build context\n",
    "    context = \"\\n\\n\".join(retrieved_chunks) if retrieved_chunks else \"\"\n",
    "\n",
    "    # Step 5: Create prompt (already provided)\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Step 6: Generate answer\n",
    "    answer = llm.generate_answer(prompt)\n",
    "\n",
    "    # Step 7: Extract sources\n",
    "    sources = [meta[\"source\"] for meta in retrieved_metadata] if retrieved_metadata else []\n",
    "\n",
    "    # Stop timer\n",
    "    elapsed_time = timer.stop()\n",
    "\n",
    "    # Step 8: Return results\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources,\n",
    "        \"contexts\": retrieved_chunks,\n",
    "        \"time\": elapsed_time\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your RAG System!\n",
    "\n",
    "Let's try asking some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== RAG ANSWER ========================\n",
      "\n",
      "QUESTION: What can I make with Chicken and Rice\n",
      "\n",
      "ANSWER:\n",
      " Arroz Con Gandules (Puerto Rican Rice with Pigeon Peas) is a dish that includes chicken, rice, pigeon peas, adobo seasoning blend, sazÃ³n flavoring blend, cilantro, and olives. You can find the recipe in the provided context.\n",
      "\n",
      "SOURCES: arrozConGandules.txt, pastelon.txt\n",
      "TIME: 74.24 seconds\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test question 1\n",
    "result = rag_query(\"What can I make with Chicken and Rice\")\n",
    "\n",
    "# Pretty print the result\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== RAG ANSWER ========================\n",
      "\n",
      "QUESTION: How do you bake Pan de Agua?\n",
      "\n",
      "ANSWER:\n",
      " Pan de agua is baked by placing the proofed dough in a cold oven set above a pan of boiling water. The oven temperature is then raised and the bread is baked until golden brown.\n",
      "\n",
      "SOURCES: panDeAgua.txt\n",
      "TIME: 72.27 seconds\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Try your own question!\n",
    "my_question = \"How do you bake Pan de Agua?\"  # Change this!\n",
    "\n",
    "result = rag_query(my_question)\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #5: Create Test Dataset\n",
    "\n",
    "**Your Task:** Create a list of test questions to evaluate your RAG system.\n",
    "\n",
    "**What to do:**\n",
    "1. Think of 10 questions your documents can answer\n",
    "2. For each question, write the expected answer\n",
    "3. Store them in a structured format\n",
    "\n",
    "**Python concepts:** Lists, dictionaries, data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 10 test questions\n",
      "\n",
      "Example question:\n",
      "  Q: What are the main ingredients in arroz con gandules?\n",
      "  Expected: rice, pigeon peas (gandules), sofrito, sazÃ³n, tomato sauce, olives, and pork or ham\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create your test dataset!\n",
    "# Creating 10 recipe-related test questions based on the documents in docs/text\n",
    "\n",
    "test_questions = [\n",
    "    {\n",
    "        'question': 'What are the main ingredients in arroz con gandules?',\n",
    "        'expected_answer': 'rice, pigeon peas (gandules), sofrito, sazÃ³n, tomato sauce, olives, and pork or ham',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How long should pernil be marinated before cooking?',\n",
    "        'expected_answer': 'at least several hours, but preferably overnight or up to 24 hours',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'At what temperature and for how long do you roast pernil?',\n",
    "        'expected_answer': 'several hours at around 300â€“350Â°F until tender and the skin gets crispy',\n",
    "        'category': 'procedural'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is sofrito used for in Puerto Rican cooking?',\n",
    "        'expected_answer': 'a flavor base used to season dishes like arroz con gandules, stews, and beans',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How do you assemble a pastelÃ³n?',\n",
    "        'expected_answer': 'layer fried sweet plantains with seasoned ground beef and cheese, then bake',\n",
    "        'category': 'procedural'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What ingredients are traditionally used to make coquito?',\n",
    "        'expected_answer': 'coconut milk, cream of coconut, rum, spices, sweetened condensed milk, and vanilla',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What fillings are commonly used in empanadillas?',\n",
    "        'expected_answer': 'seasoned ground beef, chicken, cheese, or other savory fillings',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What ingredients are blended to make homemade sofrito?',\n",
    "        'expected_answer': 'peppers, onions, garlic, cilantro, recao, and other herbs',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Which recipes from this collection are commonly served during holidays?',\n",
    "        'expected_answer': 'pernil, arroz con gandules, pastelÃ³n, and coquito',\n",
    "        'category': 'inferential'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How should leftover arroz con gandules be reheated?',\n",
    "        'expected_answer': 'reheat gently with a little water on the stove or microwaved while covered',\n",
    "        'category': 'procedural'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(test_questions)} test questions\")\n",
    "print(f\"\\nExample question:\")\n",
    "print(f\"  Q: {test_questions[0]['question']}\")\n",
    "print(f\"  Expected: {test_questions[0]['expected_answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #6: Calculate Evaluation Metrics\n",
    "\n",
    "**Your Task:** Write functions to measure how well your RAG system performs.\n",
    "\n",
    "**Python concepts:** Functions, calculations, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Evaluation functions defined!\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_latency(results: List[Dict]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate average response time.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries (each has 'time' field)\n",
    "\n",
    "    Returns:\n",
    "        Average time in seconds\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return 0.0\n",
    "\n",
    "    # Extract all 'time' values and sum them\n",
    "    total_time = sum(result.get('time', 0) for result in results)\n",
    "\n",
    "    # Divide by number of results\n",
    "    average_time = total_time / len(results)\n",
    "\n",
    "    return average_time\n",
    "\n",
    "\n",
    "def count_successful_retrievals(results: List[Dict]) -> int:\n",
    "    \"\"\"\n",
    "    Count how many queries successfully retrieved context.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        Number of successful retrievals\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "\n",
    "    for result in results:\n",
    "        # 'contexts' should be a non-empty list\n",
    "        if result.get('contexts'):\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def get_all_sources(results: List[Dict]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get unique list of all sources used.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of unique source filenames\n",
    "    \"\"\"\n",
    "    all_sources = set()\n",
    "\n",
    "    for result in results:\n",
    "        for source in result.get('sources', []):\n",
    "            all_sources.add(source)\n",
    "\n",
    "    return list(all_sources)\n",
    "\n",
    "\n",
    "print(\"âœ“ Evaluation functions defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #7: Run Complete Evaluation\n",
    "\n",
    "**Your Task:** Test your RAG system with all test questions and calculate metrics.\n",
    "\n",
    "**Python concepts:** Loops, function calls, data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(test_questions: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run RAG system on all test questions.\n",
    "\n",
    "    Args:\n",
    "        test_questions: List of test question dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of result dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Create empty list for results\n",
    "    # 2. For each test question:\n",
    "    #    - Get the question text\n",
    "    #    - Call rag_query() with the question\n",
    "    #    - Add result to results list\n",
    "    # 3. Return results\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Your code here:\n",
    "    for test in test_questions:\n",
    "        # Get question\n",
    "        # Run RAG query\n",
    "        # Store result\n",
    "        pass  # Remove this 'pass' and write your code\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run evaluation on all test questions\n",
    "print(\"Running evaluation on all test questions...\\n\")\n",
    "all_results = run_evaluation(test_questions)\n",
    "\n",
    "print(f\"\\nâœ“ Completed {len(all_results)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Display Results\n",
    "\n",
    "Show the evaluation metrics and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics using your functions\n",
    "avg_latency = calculate_average_latency(all_results)\n",
    "successful_retrievals = count_successful_retrievals(all_results)\n",
    "all_sources_used = get_all_sources(all_results)\n",
    "hit_rate = successful_retrievals / len(all_results) if all_results else 0\n",
    "\n",
    "# Display metrics\n",
    "print_separator(\"EVALUATION RESULTS\")\n",
    "print(f\"\\nTotal Questions Tested: {len(all_results)}\")\n",
    "print(f\"Successful Retrievals: {successful_retrievals}\")\n",
    "print(f\"Hit Rate: {hit_rate:.2%}\")\n",
    "print(f\"Average Latency: {avg_latency:.2f} seconds\")\n",
    "print(f\"\\nSources Used: {', '.join(all_sources_used)}\")\n",
    "print_separator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display individual results\n",
    "print(\"\\nIndividual Test Results:\\n\")\n",
    "\n",
    "for i, result in enumerate(all_results, 1):\n",
    "    print(f\"[Test {i}]\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer: {result['answer'][:200]}...\")\n",
    "    print(f\"Sources: {', '.join(set(result['sources']))}\")\n",
    "    print(f\"Time: {result['time']:.2f}s\")\n",
    "    print(\"-\" * 60)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Your Results\n",
    "\n",
    "Save your test results to a JSON file for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON file\n",
    "results_summary = {\n",
    "    'metrics': {\n",
    "        'total_questions': len(all_results),\n",
    "        'successful_retrievals': successful_retrievals,\n",
    "        'hit_rate': hit_rate,\n",
    "        'average_latency': avg_latency\n",
    "    },\n",
    "    'results': all_results\n",
    "}\n",
    "\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Results saved to 'evaluation_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations! ðŸŽ‰\n",
    "\n",
    "You've successfully built a RAG system!\n",
    "\n",
    "### What You Accomplished:\n",
    "âœ… Loaded documents from files  \n",
    "âœ… Chunked text with overlap  \n",
    "âœ… Created a RAG query pipeline  \n",
    "âœ… Built a test dataset  \n",
    "âœ… Calculated evaluation metrics  \n",
    "âœ… Generated a results report  \n",
    "\n",
    "### Next Steps:\n",
    "- Try different chunk sizes and overlaps\n",
    "- Add more test questions\n",
    "- Experiment with different values for `top_k`\n",
    "- Analyze which questions work best\n",
    "- Write up your findings in a report\n",
    "\n",
    "### For Your Report:\n",
    "1. Describe your document collection\n",
    "2. Explain your chunking strategy\n",
    "3. Present your evaluation metrics\n",
    "4. Show examples of good and bad answers\n",
    "5. Discuss what you learned\n",
    "\n",
    "Great job! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag3_313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
